{
      "cells": [
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "e7bab540",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import os\n",
                        "from pathlib import Path\n",
                        "from typing import Any\n",
                        "from pydantic import Field, BaseModel\n",
                        "import httpx\n",
                        "\n",
                        "from grasp_agents import (\n",
                        "    BaseTool,\n",
                        "    LLMAgent,\n",
                        "    RunContext,\n",
                        "    Packet,\n",
                        "    ImageData,\n",
                        "    Messages,\n",
                        ")\n",
                        "from grasp_agents.typing.events import (\n",
                        "    CompletionChunkEvent,\n",
                        "    CompletionEvent,\n",
                        "    ProcPacketOutputEvent,\n",
                        ")\n",
                        "from grasp_agents.openai import OpenAILLM, OpenAILLMSettings\n",
                        "from grasp_agents.litellm import LiteLLM, LiteLLMSettings\n",
                        "from grasp_agents.grasp_logging import setup_logging\n",
                        "from grasp_agents.utils import get_timestamp\n",
                        "from grasp_agents.workflow.sequential_workflow import SequentialWorkflow\n",
                        "from grasp_agents.cloud_llm import APIProvider\n",
                        "from grasp_agents.rate_limiting import RateLimiterC\n",
                        "from grasp_agents.printer import print_event_stream"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "ff8e7ade",
                  "metadata": {},
                  "source": [
                        "Set up logging to write to the console and/or file"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "365844a1",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "PACKAGE_DIR = Path.cwd()\n",
                        "LOGGING_DIR = Path.cwd() / \"data/multiagent/logs\""
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "a25cd25a",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "LOGGING_CFG_PATH = PACKAGE_DIR / \"configs/logging/default.yaml\"\n",
                        "setup_logging(\n",
                        "    LOGGING_DIR / f\"grasp_agents_demo_{get_timestamp()}.log\", LOGGING_CFG_PATH\n",
                        ")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "81717d62",
                  "metadata": {},
                  "source": [
                        "Paths to images used in the demo"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "323297c7",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "IMG_1_URL = \"https://www.simplilearn.com/ice9/free_resources_article_thumb/Expressions_In_C_2.PNG\"\n",
                        "IMG_2_PATH = PACKAGE_DIR / \"src/grasp_agents/examples/data/expr.jpeg\""
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "7c94ca5c",
                  "metadata": {},
                  "source": [
                        "Utils"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "c1172ac3",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "def print_single_output(out: Any) -> None:\n",
                        "    print(f\"\\n<final answer>\\n{out.payloads[0]}\\n</final answer>\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "d0a377d1",
                  "metadata": {},
                  "source": [
                        "## Simple generation with validated outputs"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "a9870df1",
                  "metadata": {},
                  "source": [
                        "Output type validation"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "2647787f",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# list[int] is the output type used to validate the output\n",
                        "chatbot = LLMAgent[None, list[int], None](\n",
                        "    name=\"chatbot\",\n",
                        "    llm=LiteLLM(model_name=\"gpt-4.1\", llm_settings=LiteLLMSettings(logprobs=True)),\n",
                        ")\n",
                        "\n",
                        "# This initialises printer and usage tracker\n",
                        "ctx = RunContext[None](log_messages=True)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "8678f82d",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Code block delimiters are stripped from the output\n",
                        "out = await chatbot.run(\n",
                        "    \"Output a list of 3 integers from 0 to 10 as a python array, no talking\",\n",
                        "    ctx=ctx,\n",
                        ")\n",
                        "print_single_output(out)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "51ed7a04",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "ctx.usage_tracker.usages"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "09ef590f",
                  "metadata": {},
                  "source": [
                        "Completion data (e.g. log probs) per agent can be accessed via RunContext:"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "154912cd",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# ctx.completions"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "73766702",
                  "metadata": {},
                  "source": [
                        "Streaming with reasoning"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "74606c9f",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "chatbot = LLMAgent[None, list[int], None](\n",
                        "    name=\"chatbot\",\n",
                        "    llm=LiteLLM(\n",
                        "        model_name=\"claude-sonnet-4-20250514\",\n",
                        "        llm_settings=LiteLLMSettings(reasoning_effort=\"low\"),\n",
                        "    ),\n",
                        ")\n",
                        "ctx = RunContext[None](log_messages=False)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "2911fb64",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "async for event in print_event_stream(\n",
                        "    chatbot.run_stream(\n",
                        "        \"Output a list of 30 integers from 0 to 10 as a python array. \"\n",
                        "        \"No code or talking.\",\n",
                        "        ctx=ctx,\n",
                        "    )\n",
                        "):\n",
                        "    if isinstance(event, ProcPacketOutputEvent):\n",
                        "        out = event.data"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "de3a7ddf",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "ctx.usage_tracker.usages"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "f362155c",
                  "metadata": {},
                  "source": [
                        "Output type validation with structured outputs"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "31661412",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Some providers (e.g. `openai` and `gemini`) support structured outputs.\n",
                        "# With the OpenAI API, this will require a Pydantic model to validate the output.\n",
                        "\n",
                        "from enum import StrEnum\n",
                        "\n",
                        "\n",
                        "class Selector(StrEnum):\n",
                        "    A = \"A\"\n",
                        "    B = \"B\"\n",
                        "\n",
                        "\n",
                        "class Response(BaseModel):\n",
                        "    result: list[int] = Field(..., description=\"3 random integers\")\n",
                        "    value: Selector = Field(..., description=\"Choose a value randomly\")\n",
                        "\n",
                        "\n",
                        "chatbot = LLMAgent[None, Response, None](\n",
                        "    name=\"chatbot\",\n",
                        "    llm=LiteLLM(\n",
                        "        model_name=\"gpt-4.1\",\n",
                        "        llm_settings=LiteLLMSettings(),\n",
                        "        apply_response_schema_via_provider=True,\n",
                        "        # response_schema=Response,\n",
                        "    ),\n",
                        ")\n",
                        "\n",
                        "# By default, response_schema is set to the output type of the agent (Response)\n",
                        "# In some cases, you may want to set it to a different type, e.g. when using\n",
                        "# custom output parsing.\n",
                        "\n",
                        "ctx = RunContext[None](log_messages=True)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "423df731",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "out = await chatbot.run(\"start\", ctx=ctx)\n",
                        "print_single_output(out)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "28387ccb",
                  "metadata": {},
                  "source": [
                        "# Chat with images"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "402138d0",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "chatbot = LLMAgent[None, str, None](name=\"chatbot\", llm=LiteLLM(model_name=\"gpt-4o\"))\n",
                        "ctx = RunContext[None](log_messages=True)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "d204005b",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "out = await chatbot.run(\"Where are you headed, stranger?\", ctx=ctx)\n",
                        "print_single_output(out)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "75a659b9",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "out = await chatbot.run(\"What did you just say, exactly?\", ctx=ctx)\n",
                        "print_single_output(out)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "52b2da45",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "out = await chatbot.run(\n",
                        "    [\"What's in this image?\", ImageData.from_path(IMG_2_PATH)], ctx=ctx\n",
                        ")\n",
                        "print_single_output(out)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "e441ca1e",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "out = await chatbot.run(\"Go on\", ctx=ctx)\n",
                        "print_single_output(out)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "d0ab7eb2",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "out = await chatbot.run([\"Try another one\", ImageData.from_url(IMG_1_URL)], ctx=ctx)\n",
                        "print_single_output(out)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "5f35439a",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "out = await chatbot.run(\"What was my first question, exactly?\", ctx=ctx)\n",
                        "print_single_output(out)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "fe068493",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "ctx.usage_tracker.total_usage"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "32247af5",
                  "metadata": {},
                  "source": [
                        "# Parallel runs with retries and rate limiting"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "8ff9a28b",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Make the LLM generate text instead of integers occasionally\n",
                        "# to emphasise the need for retries\n",
                        "\n",
                        "sys_prompt = \"\"\"\n",
                        "You are a bad math student who always adds number {added_num} to the correct result of the operation. \n",
                        "Output a single integer or its name, e.g. 'three' or '3'.\n",
                        "\"\"\"\n",
                        "\n",
                        "in_prompt = \"What is the square of {num}?\"\n",
                        "\n",
                        "\n",
                        "class RunArgs(BaseModel):\n",
                        "    added_num: int\n",
                        "\n",
                        "\n",
                        "class InputArgs(BaseModel):\n",
                        "    num: int\n",
                        "\n",
                        "\n",
                        "# Specifying int as the output type means that the agent will\n",
                        "# validate the output against this type.\n",
                        "\n",
                        "student = LLMAgent[InputArgs, int, RunArgs](\n",
                        "    name=\"student\",\n",
                        "    llm=LiteLLM(\n",
                        "        model_name=\"gpt-4.1\",\n",
                        "        llm_settings=LiteLLMSettings(temperature=1.0),\n",
                        "        # This rate limit will be applied to parallel runs of the agent\n",
                        "        rate_limiter=RateLimiterC(rpm=100),\n",
                        "    ),\n",
                        "    sys_prompt=sys_prompt,\n",
                        "    in_prompt=in_prompt,\n",
                        "    max_retries=4,\n",
                        ")\n",
                        "\n",
                        "\n",
                        "@student.add_system_prompt_builder\n",
                        "def system_prompt_builder(ctx: RunContext[RunArgs]) -> str:\n",
                        "    return sys_prompt.format(added_num=ctx.state.added_num)\n"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "51fb8f25",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "in_args = [InputArgs(num=i) for i in range(10)]"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "c9816969",
                  "metadata": {},
                  "source": [
                        "Without streaming"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "2ac96a56",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "ctx = RunContext[RunArgs](state=RunArgs(added_num=5), log_messages=True)\n",
                        "\n",
                        "out = await student.run(in_args=in_args, ctx=ctx)\n",
                        "\n",
                        "print()\n",
                        "print(*[p for p in out.payloads], sep=\"\\n\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "19e29df9",
                  "metadata": {},
                  "source": [
                        "With streaming"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "755f278c",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# ctx = RunContext[RunArgs](state=RunArgs(added_num=5), log_messages=False)\n",
                        "\n",
                        "# async for event in print_event_stream(student.run_stream(in_args=in_args, ctx=ctx)):\n",
                        "#     if isinstance(event, ProcPacketOutputEvent):\n",
                        "#         out = event.data"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "a8b2df86",
                  "metadata": {},
                  "source": [
                        "# ReAct agent loop with streaming"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "cf879624",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "sys_prompt_react = \"\"\"\n",
                        "Your task is to suggest an exciting stats problem to the student. \n",
                        "You should first ask the student about their education, interests, and preferences, then suggest a problem tailored specifically to them. \n",
                        "\n",
                        "# Instructions\n",
                        "* Use the provided tool to ask questions.\n",
                        "* Ask questions one by one.\n",
                        "* Provide your thinking before asking a question and after receiving a reply.\n",
                        "* Do not include your exact question as part of your thinking.\n",
                        "* The problem must have all the necessary data.\n",
                        "* Use the final answer tool to provide the problem.\n",
                        "\"\"\""
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "d8b5401f",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Tool input must be a Pydantic model to infer the JSON schema used by the LLM APIs\n",
                        "class TeacherQuestion(BaseModel):\n",
                        "    question: str\n",
                        "\n",
                        "\n",
                        "StudentReply = str\n",
                        "\n",
                        "\n",
                        "ask_student_tool_description = \"\"\"\n",
                        "\"Ask the student a question and get their reply.\"\n",
                        "\n",
                        "Args:\n",
                        "    question: str\n",
                        "        The question to ask the student.\n",
                        "Returns:\n",
                        "    reply: str\n",
                        "        The student's reply to the question.\n",
                        "\"\"\"\n",
                        "\n",
                        "\n",
                        "class AskStudentTool(BaseTool[TeacherQuestion, StudentReply, Any]):\n",
                        "    name: str = \"ask_student\"\n",
                        "    description: str = ask_student_tool_description\n",
                        "\n",
                        "    async def run(\n",
                        "        self, inp: TeacherQuestion, ctx: RunContext[Any] | None = None\n",
                        "    ) -> StudentReply:\n",
                        "        return input(inp.question)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "dd35a6ac",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "class Problem(BaseModel):\n",
                        "    problem: str\n",
                        "\n",
                        "\n",
                        "teacher = LLMAgent[None, Problem, None](\n",
                        "    name=\"teacher\",\n",
                        "    llm=LiteLLM(\n",
                        "        model_name=\"gpt-4.1\",\n",
                        "        # model_name=\"claude-sonnet-4-20250514\",\n",
                        "        # llm_settings=LiteLLMSettings(reasoning_effort=\"low\"),\n",
                        "    ),\n",
                        "    tools=[AskStudentTool()],\n",
                        "    react_mode=True,\n",
                        "    final_answer_as_tool_call=True,\n",
                        "    sys_prompt=sys_prompt_react,\n",
                        ")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "2b45956c",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "ctx = RunContext[None](log_messages=False)\n",
                        "\n",
                        "events = []\n",
                        "problem: Problem\n",
                        "async for event in print_event_stream(teacher.run_stream(\"start\", ctx=ctx)):\n",
                        "    if isinstance(event, ProcPacketOutputEvent):\n",
                        "        problem = event.data.payloads[0]\n",
                        "    events.append(event)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "0482f411",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "problem"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "f1e6be13",
                  "metadata": {},
                  "source": [
                        "# Sequential workflow "
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "829b6c96",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Input arguments are passed to the agent dynamically (e.g. by other agents)\n",
                        "from grasp_agents.typing.content import Content\n",
                        "\n",
                        "\n",
                        "# Global state is used to store data that is shared between runs of the agent.\n",
                        "class State(BaseModel):\n",
                        "    b: int\n",
                        "    c: int\n",
                        "\n",
                        "\n",
                        "class AddInputArgs(BaseModel):\n",
                        "    a: int = Field(..., description=\"First number to add.\")\n",
                        "\n",
                        "\n",
                        "class AddResponse(BaseModel):\n",
                        "    a_plus_b: int\n",
                        "\n",
                        "\n",
                        "add_in_prompt = \"Add {a} and {b}. Your only output is the resulting number.\"\n",
                        "\n",
                        "\n",
                        "add_agent = LLMAgent[AddInputArgs, AddResponse, State](\n",
                        "    name=\"add_agent\",\n",
                        "    llm=LiteLLM(model_name=\"gpt-4.1\"),\n",
                        "    in_prompt=add_in_prompt,\n",
                        "    # Reset message history to system prompt (if provided) before each run\n",
                        "    reset_memory_on_run=True,\n",
                        ")\n",
                        "\n",
                        "\n",
                        "@add_agent.add_input_content_builder\n",
                        "def build_add_input_content(\n",
                        "    in_args: AddInputArgs | None = None, ctx: RunContext[State] | None = None\n",
                        ") -> Content:\n",
                        "    return Content.from_formatted_prompt(\n",
                        "        add_agent.in_prompt, a=in_args.a, b=ctx.state.b\n",
                        "    )\n",
                        "\n",
                        "\n",
                        "@add_agent.add_output_parser\n",
                        "def parse_add_output(conversation: Messages, **kwargs: Any) -> AddResponse:\n",
                        "    return AddResponse(a_plus_b=int(str(conversation[-1].content)))"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "75c68936",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "class MultiplyResponse(BaseModel):\n",
                        "    c_a_plus_b: int\n",
                        "\n",
                        "\n",
                        "multiply_in_prompt = (\n",
                        "    \"Multiply {a_plus_b} by {c}. Your only output is the resulting number.\"\n",
                        ")\n",
                        "\n",
                        "multiply_agent = LLMAgent[AddResponse, MultiplyResponse, State](\n",
                        "    name=\"multiply_agent\",\n",
                        "    llm=LiteLLM(model_name=\"gpt-4.1\"),\n",
                        "    in_prompt=multiply_in_prompt,\n",
                        "    reset_memory_on_run=True,\n",
                        ")\n",
                        "\n",
                        "\n",
                        "# Need a custom input content maker to use the global state\n",
                        "@multiply_agent.add_input_content_builder\n",
                        "def build_multiply_input_content(\n",
                        "    in_args: AddResponse | None = None, ctx: RunContext[State] | None = None\n",
                        ") -> Content:\n",
                        "    return Content.from_formatted_prompt(\n",
                        "        multiply_agent.in_prompt, a_plus_b=in_args.a_plus_b, c=ctx.state.c\n",
                        "    )\n",
                        "\n",
                        "\n",
                        "@multiply_agent.add_output_parser\n",
                        "def parse_multiply_output(conversation: Messages, **kwargs: Any) -> MultiplyResponse:\n",
                        "    return MultiplyResponse(c_a_plus_b=int(str(conversation[-1].content)))"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "72f58f89",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "seq_agent = SequentialWorkflow[AddInputArgs, MultiplyResponse, State](\n",
                        "    name=\"seq_agent\", subprocs=[add_agent, multiply_agent]\n",
                        ")"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "aeae20e3",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "state = State(b=3, c=6)\n",
                        "ctx = RunContext[State](state=state, log_messages=True)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "bec285cb",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# Can pass batched arguments\n",
                        "out = await seq_agent.run(in_args=AddInputArgs(a=2), ctx=ctx)\n",
                        "# out = await seq_agent.run(in_args=[AddInputArgs(a=2), AddInputArgs(a=3)], ctx=ctx)\n",
                        "\n",
                        "print_single_output(out)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "5134497a",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# ctx = RunContext[State](state=state, log_messages=False)\n",
                        "# events = []\n",
                        "# async for event in print_event_stream(\n",
                        "#     seq_agent.run_stream(in_args=AddInputArgs(a=2), ctx=ctx)\n",
                        "# ):\n",
                        "#     events.append(event)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "dbf3874c",
                  "metadata": {},
                  "source": [
                        "# Agents as tools"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "457c5961",
                  "metadata": {},
                  "source": [
                        "When agents are used as tools, their `in_args` become the tool inputs.\n",
                        "\n",
                        "This is how one can implement a manager + helpers architecture."
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "aab91548",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "seq_tool = seq_agent.as_tool(\n",
                        "    tool_name=\"seq_agent_tool\",\n",
                        "    tool_description=(\n",
                        "        \"A sequential agent that adds 3 to a given integer, \"\n",
                        "        \"then multiplies the result by 5.\"\n",
                        "    ),\n",
                        ")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "802b16b4",
                  "metadata": {},
                  "source": [
                        "The JSON schema of `in_args` is preserved:"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "bd34c0c6",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "seq_tool.in_type.model_json_schema()"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "55d19e9a",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "await seq_tool(a=15, ctx=ctx)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "2568daad",
                  "metadata": {},
                  "source": [
                        "# Teacher / students"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "0f214392",
                  "metadata": {},
                  "source": [
                        "A more advanced example of multi-agent debate, where agents communicate using the actor model."
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "f5002018",
                  "metadata": {},
                  "source": [
                        "Communication schemas"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "251ad59f",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from grasp_agents.runner import Runner"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "78653648",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from typing import Literal\n",
                        "\n",
                        "\n",
                        "TeacherRecipient = Literal[\"*END*\", \"teacher\", \"student1\", \"student2\"]\n",
                        "\n",
                        "\n",
                        "# Teacher can choose which students to send the message to\n",
                        "class TeacherExplanation(BaseModel):\n",
                        "    explanation: str\n",
                        "    selected_recipients: list[TeacherRecipient] = Field(\n",
                        "        default_factory=list,\n",
                        "        description=\"Recipients selected by the teacher.\",\n",
                        "    )\n",
                        "\n",
                        "\n",
                        "# Students can only ask questions to the teacher\n",
                        "class StudentQuestion(BaseModel):\n",
                        "    question: str = Field(\n",
                        "        ...,\n",
                        "        description=\"The question to ask the teacher.\",\n",
                        "    )"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "27d4d626",
                  "metadata": {},
                  "source": [
                        "#### Teacher"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "7a17ad69",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "teacher_sys_prompt = \"\"\"\n",
                        "You are a teacher explaining quantum gravity to a 2-year old baby (named 'student1') and a 30-year old graphic designer (named student2). \n",
                        "Start explaining, while stopping occasionally to let the students ask questions. \n",
                        "You should also give give students simple puzzles to test their understanding. \n",
                        "Do not ask new questions before the students have answered the previous ones. \n",
                        "To indicate to whom you are addressing your message, you must specify the recipients as a list of selected student names. \n",
                        "When students have no more questions, finish the conversation with a SINGLE message with a SINGLE recipient called *END*. \n",
                        "Do not produce multiple \"thanks\" or \"goodbye\" messages, just a single one.\n",
                        "\"\"\"\n",
                        "\n",
                        "teacher = LLMAgent[StudentQuestion, TeacherExplanation, None](\n",
                        "    name=\"teacher\",\n",
                        "    llm=LiteLLM(model_name=\"gpt-4o\", apply_response_schema_via_provider=True),\n",
                        "    sys_prompt=teacher_sys_prompt,\n",
                        "    # need to specify allowed recipients to choose from\n",
                        "    recipients=[\"*END*\", \"student1\", \"student2\"],\n",
                        ")\n",
                        "\n",
                        "\n",
                        "@teacher.add_recipient_selector\n",
                        "def select_recipients(\n",
                        "    output: TeacherExplanation, **kwargs: Any\n",
                        ") -> list[TeacherRecipient]:\n",
                        "    return output.selected_recipients"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "a26d93e0",
                  "metadata": {},
                  "source": [
                        "#### Students"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "f6865a08",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "student_sys_prompts = [\n",
                        "    \"\"\"\n",
                        "You are a 4-year old child trying to make sense of physics. \n",
                        "Your name is <student1>.\n",
                        "Talk to the teacher to understand the topic.\n",
                        "There is also another student in the class, a 30 year old graphic designer. \n",
                        "You talk to the teacher only.\n",
                        "\"\"\",\n",
                        "    \"\"\"\n",
                        "You are a 30-year old experienced graphic designer curious about physics. \n",
                        "Your name is <student2>.\n",
                        "Ask questions to the teacher until you understand the topic. \n",
                        "Attempt to answer the teacher's questions, but if you don't understand,\n",
                        "ask for clarification. \n",
                        "There is also another student in the class, a 4-year old child.\n",
                        "You talk to the teacher only.\n",
                        "\"\"\",\n",
                        "]\n",
                        "\n",
                        "\n",
                        "def make_student_agent(name: str, sys_prompt: str):\n",
                        "    student = LLMAgent[TeacherExplanation, StudentQuestion, None](\n",
                        "        name=name,\n",
                        "        llm=LiteLLM(model_name=\"gpt-4o\", apply_response_schema_via_provider=True),\n",
                        "        sys_prompt=sys_prompt,\n",
                        "        recipients=[\"teacher\"],\n",
                        "    )\n",
                        "\n",
                        "    @student.add_output_parser\n",
                        "    def parse_student_output(conversation: Messages, **kwargs: Any) -> StudentQuestion:\n",
                        "        return StudentQuestion(question=f\"<{name}>: \" + str(conversation[-1].content))\n",
                        "\n",
                        "    return student\n",
                        "\n",
                        "\n",
                        "student1 = make_student_agent(\"student1\", student_sys_prompts[0])\n",
                        "student2 = make_student_agent(\"student2\", student_sys_prompts[1])"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "79aa9608",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "ctx = RunContext[None](log_messages=True, color_messages_by=\"agent\")\n",
                        "runner = Runner(entry_proc=teacher, procs=[teacher, student1, student2], ctx=ctx)\n",
                        "final_result = await runner.run(chat_input=\"start\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "a8f5d700",
                  "metadata": {},
                  "source": [
                        "Streaming"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "cb2001bd",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# runner = Runner(\n",
                        "#     entry_proc=teacher, procs=[teacher, student1, student2], ctx=RunContext[None]()\n",
                        "# )\n",
                        "# events = []\n",
                        "# async for event in print_event_stream(\n",
                        "#     runner.run_stream(chat_input=\"start\"), color_by=\"agent\"\n",
                        "# ):\n",
                        "#     events.append(event)"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "id": "e845d2d2",
                  "metadata": {},
                  "source": [
                        "# Custom API providers and HTTP clients"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "id": "94146e17",
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "custom_provider = APIProvider(\n",
                        "    name=\"openrouter\",\n",
                        "    base_url=\"https://openrouter.ai/api/v1\",\n",
                        "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
                        ")\n",
                        "\n",
                        "http_client = httpx.AsyncClient(\n",
                        "    timeout=httpx.Timeout(10),\n",
                        "    limits=httpx.Limits(max_connections=10),\n",
                        ")\n",
                        "\n",
                        "chatbot = LLMAgent[None, list[int], None](\n",
                        "    name=\"chatbot\",\n",
                        "    llm=OpenAILLM(\n",
                        "        model_name=\"deepseek/deepseek-r1-0528\",\n",
                        "        api_provider=custom_provider,\n",
                        "        async_http_client=http_client,\n",
                        "    ),\n",
                        ")\n",
                        "\n",
                        "\n",
                        "ctx = RunContext[None](log_messages=True)\n",
                        "out = await chatbot.run(\n",
                        "    \"Output a list of 3 integers from 0 to 10 as a python array, no talking\",\n",
                        "    ctx=ctx,\n",
                        ")\n",
                        "print_single_output(out)"
                  ]
            }
      ],
      "metadata": {
            "kernelspec": {
                  "display_name": "grasp-agents",
                  "language": "python",
                  "name": "python3"
            },
            "language_info": {
                  "codemirror_mode": {
                        "name": "ipython",
                        "version": 3
                  },
                  "file_extension": ".py",
                  "mimetype": "text/x-python",
                  "name": "python",
                  "nbconvert_exporter": "python",
                  "pygments_lexer": "ipython3",
                  "version": "3.11.9"
            }
      },
      "nbformat": 4,
      "nbformat_minor": 5
}
