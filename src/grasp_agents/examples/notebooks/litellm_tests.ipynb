{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa5d00a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "import litellm\n",
    "# from grasp_agents.grasp_logging import setup_logging\n",
    "# from grasp_agents.utils import get_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f83b00",
   "metadata": {},
   "source": [
    "Set up logging to write to the console and/or file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9757f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PACKAGE_DIR = Path.cwd()\n",
    "LOGGING_DIR = Path.cwd() / \"data/multiagent/logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02ab786e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'setup_logging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m LOGGING_CFG_PATH \u001b[38;5;241m=\u001b[39m PACKAGE_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigs/logging/default.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msetup_logging\u001b[49m(\n\u001b[1;32m      3\u001b[0m     LOGGING_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrasp_agents_demo_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mget_timestamp()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.log\u001b[39m\u001b[38;5;124m\"\u001b[39m, LOGGING_CFG_PATH\n\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'setup_logging' is not defined"
     ]
    }
   ],
   "source": [
    "LOGGING_CFG_PATH = PACKAGE_DIR / \"configs/logging/default.yaml\"\n",
    "setup_logging(\n",
    "    LOGGING_DIR / f\"grasp_agents_demo_{get_timestamp()}.log\", LOGGING_CFG_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b741849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[REASONING] **Initiating the Analysis**\n",
      "\n",
      "I'm starting by dissecting the request. The core is explaining rainbow formation, aiming for a clear, concise paragraph. I'm focusing on key elements to ensure the explanation is easy to grasp.\n",
      "\n",
      "\n",
      "\n",
      "[OUTPUT] None\n",
      "---------------\n",
      "\n",
      "[REASONING] **Analyzing the Core Components**\n",
      "\n",
      "I've outlined the necessary ingredients for a clear explanation: sunlight, raindrops, the observer, refraction, dispersion, and reflection. I'm focusing now on constructing a sequential framework to describe how light travels through the water droplets, producing the spectrum of colors. The aim is to create a compelling narrative that is both accurate and easily understood.\n",
      "\n",
      "\n",
      "\n",
      "[OUTPUT] None\n",
      "---------------\n",
      "\n",
      "[REASONING] **Constructing the Narrative**\n",
      "\n",
      "I'm now integrating the key ingredients and process into a single, cohesive paragraph. I'm focusing on crafting a smooth flow that is easy to follow. The goal is clarity, ensuring the explanation can be understood by anyone.\n",
      "\n",
      "\n",
      "\n",
      "[OUTPUT] None\n",
      "---------------\n",
      "\n",
      "[REASONING] **Formulating the Explanation**\n",
      "\n",
      "I've structured the paragraph around the path of light, from sunlight entering the raindrops to the resulting arc of colors. I'm focusing on simplicity, emphasizing key steps like refraction, dispersion, and reflection. The ultimate aim is to create a concise, yet informative, explanation of rainbow formation for the average person.\n",
      "\n",
      "\n",
      "\n",
      "[OUTPUT] None\n",
      "---------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Delta' object has no attribute 'reasoning_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Grasp/repos/grasp-agents/.venv/lib/python3.11/site-packages/pydantic/main.py:981\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpydantic_extra\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'reasoning_content'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 35\u001b[0m\n\u001b[1;32m     29\u001b[0m delta \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdelta\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# if delta.content:  # visible answer tokens\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#     print(delta.content)  # , end=\"\", flush=True)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Anthropic inserts thinking into message.reasoning_content (1-2 early chunks)\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdelta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreasoning_content\u001b[49m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[REASONING]\u001b[39m\u001b[38;5;124m\"\u001b[39m, delta\u001b[38;5;241m.\u001b[39mreasoning_content)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(delta, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthinking_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[0;32m~/Grasp/repos/grasp-agents/.venv/lib/python3.11/site-packages/pydantic/main.py:983\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    981\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pydantic_extra[item]\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 983\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, item):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Delta' object has no attribute 'reasoning_content'"
     ]
    }
   ],
   "source": [
    "import os, itertools\n",
    "from litellm import (\n",
    "    completion,\n",
    "    stream_chunk_builder,\n",
    ")  # stream_chunk_builder is handy but optional\n",
    "\n",
    "os.environ[\"GEMINI_API_KEY\"] = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain how rainbows form in one short paragraph.\"}\n",
    "]\n",
    "\n",
    "# --- fire the request ---\n",
    "stream = completion(\n",
    "    # model=\"anthropic/claude-sonnet-4-20250514\",\n",
    "    model=\"gemini/gemini-2.5-pro\",\n",
    "    messages=messages,\n",
    "    stream=True,  # turn on token streaming :contentReference[oaicite:3]{index=3}\n",
    "    reasoning_effort=\"low\",  # ask Anthropic to return thinking blocks :contentReference[oaicite:4]{index=4}\n",
    "    stream_options={\n",
    "        \"include_usage\": True\n",
    "    },  # optional – first chunk carries token usage :contentReference[oaicite:5]{index=5}\n",
    ")\n",
    "\n",
    "# --- consume the stream ---\n",
    "chunks = []\n",
    "for chunk in stream:\n",
    "    chunks.append(chunk)\n",
    "    delta = chunk.choices[0].delta\n",
    "    # if delta.content:  # visible answer tokens\n",
    "    #     print(delta.content)  # , end=\"\", flush=True)\n",
    "\n",
    "    # Anthropic inserts thinking into message.reasoning_content (1-2 early chunks)\n",
    "\n",
    "    if delta.reasoning_content:\n",
    "        print(\"\\n[REASONING]\", delta.reasoning_content)\n",
    "\n",
    "    if getattr(delta, \"thinking_blocks\", None):\n",
    "        for tb in delta.thinking_blocks:\n",
    "            print(\"\\n[THINKING BLOCK]\", tb[\"thinking\"])\n",
    "\n",
    "    print(\"[OUTPUT]\", delta.content)\n",
    "    print(\"---------------\")\n",
    "\n",
    "combined_chunk = litellm.stream_chunk_builder(chunks)\n",
    "\n",
    "# if rc:\n",
    "#     print(\"\\n\\n[THINKING]\", rc)  # inspect or log quietly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "481ef6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_chunk._hidden_params[\"response_cost\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a0b067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': FieldInfo(annotation=Union[str, NoneType], required=True),\n",
       " 'role': FieldInfo(annotation=Literal['assistant', 'user', 'system', 'tool', 'function'], required=True),\n",
       " 'tool_calls': FieldInfo(annotation=Union[List[ChatCompletionMessageToolCall], NoneType], required=True),\n",
       " 'function_call': FieldInfo(annotation=Union[FunctionCall, NoneType], required=True),\n",
       " 'audio': FieldInfo(annotation=Union[ChatCompletionAudioResponse, NoneType], required=False, default=None),\n",
       " 'reasoning_content': FieldInfo(annotation=Union[str, NoneType], required=False, default=None),\n",
       " 'thinking_blocks': FieldInfo(annotation=Union[List[Union[ChatCompletionThinkingBlock, ChatCompletionRedactedThinkingBlock]], NoneType], required=False, default=None),\n",
       " 'provider_specific_fields': FieldInfo(annotation=Union[Dict[str, Any], NoneType], required=False, default=None, exclude=True),\n",
       " 'annotations': FieldInfo(annotation=Union[List[ChatCompletionAnnotation], NoneType], required=False, default=None)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(combined_chunk.choices[0].message).model_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf531e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa639f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205ae2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeb1017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<thought>**Breaking Down Multiplication**\n",
      "\n",
      "I'm currently focused on the standard multiplication algorithm, specifically long multiplication, as the most appropriate method. I've broken down the request into understanding the specific calculation needed (21 x 37) and recalling the necessary steps of the long multiplication process, such as writing the numbers vertically and multiplying digits individually. The first phase of my analysis is now complete and I am moving towards performing the multiplication step by step.\n",
      "\n",
      "\n",
      "**Deciphering Partial Products**\n",
      "\n",
      "I've completed the initial setup and am now deep in the core calculations. The first partial product, 147, from multiplying 21 by 7, is correctly placed. I'm focusing on the next step: multiplying 21 by 30 (the tens digit). I'm careful to use the zero placeholder for correct place value alignment. I'm on track to find the second partial product.\n",
      "\n",
      "\n",
      "**Adding Partial Products**\n",
      "\n",
      "I've completed the initial multiplication steps and arrived at the two partial products: 147 and 630. My focus now is on the final addition. I'm carefully adding the digits in each column, ensuring proper place value alignment for the final answer. I am making sure not to make any calculation mistakes during the addition of the partial products. The process is almost complete.\n",
      "\n",
      "\n",
      "</thought>Let's solve $21 \\times 37$ step-by-step using the standard multiplication method.\n",
      "\n",
      "**Step 1: Set up the multiplication.**\n",
      "Write the numbers one above the other, aligning them by their place values.\n",
      "\n",
      "```\n",
      "  21\n",
      "x 37\n",
      "----\n",
      "```\n",
      "\n",
      "**Step 2: Multiply the top number (21) by the ones digit of the bottom number (7).**\n",
      "\n",
      "*   Multiply 7 by 1: $7 \\times 1 = 7$. Write 7 in the ones place below the line.\n",
      "*   Multiply 7 by 2: $7 \\times 2 = 14$. Write 14 next to the 7.\n",
      "\n",
      "```\n",
      "  21\n",
      "x 37\n",
      "----\n",
      " 147  (This is 7 x 21)\n",
      "```\n",
      "\n",
      "**Step 3: Multiply the top number (21) by the tens digit of the bottom number (3).**\n",
      "Since 3 is in the tens place, we are essentially multiplying by 30. To account for this, place a 0 in the ones place on the next line before you start multiplying.\n",
      "\n",
      "*   Write down a 0 in the ones place.\n",
      "*   Multiply 3 by 1: $3 \\times 1 = 3$. Write 3 in the tens place.\n",
      "*   Multiply 3 by 2: $3 \\times 2 = 6$. Write 6 in the hundreds place.\n",
      "\n",
      "```\n",
      "  21\n",
      "x 37\n",
      "----\n",
      " 147\n",
      "630   (This is 30 x 21)\n",
      "```\n",
      "\n",
      "**Step 4: Add the results from Step 2 and Step 3.**\n",
      "\n",
      "```\n",
      "  147\n",
      "+ 630\n",
      "-----\n",
      "  777\n",
      "```\n",
      "\n",
      "*   Add the ones column: $7 + 0 = 7$\n",
      "*   Add the tens column: $4 + 3 = 7$\n",
      "*   Add the hundreds column: $1 + 6 = 7$\n",
      "\n",
      "**Final Answer:**\n",
      "$21 \\times 37 = 777$"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os, sys\n",
    "\n",
    "os.environ[\"GEMINI_API_KEY\"] = os.getenv(\"GEMINI_API_KEY\")\n",
    "client = OpenAI(\n",
    "    api_key=os.environ[\"GEMINI_API_KEY\"],\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    ")\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    stream=True,\n",
    "    # reasoning_effort=\"low\",  # maps to a 1 k-token thinking budget\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Solve 21×37 step-by-step\"}],\n",
    "    extra_body={\n",
    "        \"extra_body\": {\n",
    "            \"google\": {\n",
    "                \"thinking_config\": {\"thinking_budget\": 800, \"include_thoughts\": True}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    delta = chunk.choices[0].delta\n",
    "    if getattr(delta, \"thought\", None):  # reasoning tokens\n",
    "        print(\"\\n[THINK]\", delta.thought, flush=True)\n",
    "    if delta.content:  # answer tokens\n",
    "        print(delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5498ac4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['omni-moderation-latest',\n",
       " 'omni-moderation-latest-intents',\n",
       " 'omni-moderation-2024-09-26',\n",
       " 'gpt-4',\n",
       " 'gpt-4.1',\n",
       " 'gpt-4.1-2025-04-14',\n",
       " 'gpt-4.1-mini',\n",
       " 'gpt-4.1-mini-2025-04-14',\n",
       " 'gpt-4.1-nano',\n",
       " 'gpt-4.1-nano-2025-04-14',\n",
       " 'gpt-4o',\n",
       " 'gpt-4o-search-preview-2025-03-11',\n",
       " 'gpt-4o-search-preview',\n",
       " 'gpt-4.5-preview',\n",
       " 'gpt-4.5-preview-2025-02-27',\n",
       " 'gpt-4o-audio-preview',\n",
       " 'gpt-4o-audio-preview-2024-12-17',\n",
       " 'gpt-4o-audio-preview-2024-10-01',\n",
       " 'gpt-4o-audio-preview-2025-06-03',\n",
       " 'gpt-4o-mini-audio-preview',\n",
       " 'gpt-4o-mini-audio-preview-2024-12-17',\n",
       " 'gpt-4o-mini',\n",
       " 'gpt-4o-mini-search-preview-2025-03-11',\n",
       " 'gpt-4o-mini-search-preview',\n",
       " 'gpt-4o-mini-2024-07-18',\n",
       " 'codex-mini-latest',\n",
       " 'o1-pro',\n",
       " 'o1-pro-2025-03-19',\n",
       " 'o1',\n",
       " 'o1-mini',\n",
       " 'o3-deep-research',\n",
       " 'o3-deep-research-2025-06-26',\n",
       " 'o3-pro',\n",
       " 'o3-pro-2025-06-10',\n",
       " 'o3',\n",
       " 'o3-2025-04-16',\n",
       " 'o3-mini',\n",
       " 'o3-mini-2025-01-31',\n",
       " 'o4-mini',\n",
       " 'o4-mini-deep-research',\n",
       " 'o4-mini-deep-research-2025-06-26',\n",
       " 'o4-mini-2025-04-16',\n",
       " 'o1-mini-2024-09-12',\n",
       " 'o1-preview',\n",
       " 'o1-preview-2024-09-12',\n",
       " 'o1-2024-12-17',\n",
       " 'chatgpt-4o-latest',\n",
       " 'gpt-4o-2024-05-13',\n",
       " 'gpt-4o-2024-08-06',\n",
       " 'gpt-4o-2024-11-20',\n",
       " 'gpt-4o-realtime-preview-2024-10-01',\n",
       " 'gpt-4o-realtime-preview',\n",
       " 'gpt-4o-realtime-preview-2024-12-17',\n",
       " 'gpt-4o-mini-realtime-preview',\n",
       " 'gpt-4o-mini-realtime-preview-2024-12-17',\n",
       " 'gpt-4-turbo-preview',\n",
       " 'gpt-4-0314',\n",
       " 'gpt-4-0613',\n",
       " 'gpt-4-32k',\n",
       " 'gpt-4-32k-0314',\n",
       " 'gpt-4-32k-0613',\n",
       " 'gpt-4-turbo',\n",
       " 'gpt-4-turbo-2024-04-09',\n",
       " 'gpt-4-1106-preview',\n",
       " 'gpt-4-0125-preview',\n",
       " 'gpt-4-vision-preview',\n",
       " 'gpt-4-1106-vision-preview',\n",
       " 'gpt-3.5-turbo',\n",
       " 'gpt-3.5-turbo-0301',\n",
       " 'gpt-3.5-turbo-0613',\n",
       " 'gpt-3.5-turbo-1106',\n",
       " 'gpt-3.5-turbo-0125',\n",
       " 'gpt-3.5-turbo-16k',\n",
       " 'gpt-3.5-turbo-16k-0613',\n",
       " 'text-embedding-3-large',\n",
       " 'text-embedding-3-small',\n",
       " 'text-embedding-ada-002',\n",
       " 'text-embedding-ada-002-v2',\n",
       " 'text-moderation-stable',\n",
       " 'text-moderation-007',\n",
       " 'text-moderation-latest',\n",
       " '256-x-256/dall-e-2',\n",
       " '512-x-512/dall-e-2',\n",
       " '1024-x-1024/dall-e-2',\n",
       " 'hd/1024-x-1792/dall-e-3',\n",
       " 'hd/1792-x-1024/dall-e-3',\n",
       " 'hd/1024-x-1024/dall-e-3',\n",
       " 'standard/1024-x-1792/dall-e-3',\n",
       " 'standard/1792-x-1024/dall-e-3',\n",
       " 'standard/1024-x-1024/dall-e-3',\n",
       " 'gpt-image-1',\n",
       " 'low/1024-x-1024/gpt-image-1',\n",
       " 'medium/1024-x-1024/gpt-image-1',\n",
       " 'high/1024-x-1024/gpt-image-1',\n",
       " 'low/1024-x-1536/gpt-image-1',\n",
       " 'medium/1024-x-1536/gpt-image-1',\n",
       " 'high/1024-x-1536/gpt-image-1',\n",
       " 'low/1536-x-1024/gpt-image-1',\n",
       " 'medium/1536-x-1024/gpt-image-1',\n",
       " 'high/1536-x-1024/gpt-image-1',\n",
       " 'gpt-4o-transcribe',\n",
       " 'gpt-4o-mini-transcribe',\n",
       " 'whisper-1',\n",
       " 'tts-1',\n",
       " 'tts-1-hd',\n",
       " 'gpt-4o-mini-tts',\n",
       " 'ft:davinci-002',\n",
       " 'ft:babbage-002',\n",
       " 'babbage-002',\n",
       " 'davinci-002',\n",
       " 'gpt-3.5-turbo-instruct',\n",
       " 'gpt-3.5-turbo-instruct-0914',\n",
       " 'claude-instant-1',\n",
       " 'claude-instant-1.2',\n",
       " 'claude-2',\n",
       " 'claude-2.1',\n",
       " 'claude-3-haiku-20240307',\n",
       " 'claude-3-5-haiku-20241022',\n",
       " 'claude-3-5-haiku-latest',\n",
       " 'claude-3-opus-latest',\n",
       " 'claude-3-opus-20240229',\n",
       " 'claude-3-sonnet-20240229',\n",
       " 'claude-3-5-sonnet-latest',\n",
       " 'claude-3-5-sonnet-20240620',\n",
       " 'claude-opus-4-20250514',\n",
       " 'claude-sonnet-4-20250514',\n",
       " 'claude-4-opus-20250514',\n",
       " 'claude-4-sonnet-20250514',\n",
       " 'claude-3-7-sonnet-latest',\n",
       " 'claude-3-7-sonnet-20250219',\n",
       " 'claude-3-5-sonnet-20241022',\n",
       " 'openrouter/deepseek/deepseek-r1-0528',\n",
       " 'openrouter/deepseek/deepseek-r1',\n",
       " 'openrouter/deepseek/deepseek-chat',\n",
       " 'openrouter/deepseek/deepseek-coder',\n",
       " 'openrouter/microsoft/wizardlm-2-8x22b:nitro',\n",
       " 'openrouter/google/gemini-2.5-pro',\n",
       " 'openrouter/google/gemini-pro-1.5',\n",
       " 'openrouter/google/gemini-2.0-flash-001',\n",
       " 'openrouter/google/gemini-2.5-flash',\n",
       " 'openrouter/mistralai/mixtral-8x22b-instruct',\n",
       " 'openrouter/cohere/command-r-plus',\n",
       " 'openrouter/databricks/dbrx-instruct',\n",
       " 'openrouter/anthropic/claude-3-haiku',\n",
       " 'openrouter/anthropic/claude-3-5-haiku',\n",
       " 'openrouter/anthropic/claude-3-haiku-20240307',\n",
       " 'openrouter/anthropic/claude-3-5-haiku-20241022',\n",
       " 'openrouter/anthropic/claude-3.5-sonnet',\n",
       " 'openrouter/anthropic/claude-3.5-sonnet:beta',\n",
       " 'openrouter/anthropic/claude-3.7-sonnet',\n",
       " 'openrouter/anthropic/claude-3.7-sonnet:beta',\n",
       " 'openrouter/anthropic/claude-3-sonnet',\n",
       " 'openrouter/anthropic/claude-sonnet-4',\n",
       " 'openrouter/mistralai/mistral-large',\n",
       " 'openrouter/mistralai/mistral-small-3.1-24b-instruct',\n",
       " 'openrouter/mistralai/mistral-small-3.2-24b-instruct',\n",
       " 'openrouter/cognitivecomputations/dolphin-mixtral-8x7b',\n",
       " 'openrouter/google/gemini-pro-vision',\n",
       " 'openrouter/fireworks/firellava-13b',\n",
       " 'openrouter/meta-llama/llama-3-8b-instruct:free',\n",
       " 'openrouter/meta-llama/llama-3-8b-instruct:extended',\n",
       " 'openrouter/meta-llama/llama-3-70b-instruct:nitro',\n",
       " 'openrouter/meta-llama/llama-3-70b-instruct',\n",
       " 'openrouter/openai/o1',\n",
       " 'openrouter/openai/o1-mini',\n",
       " 'openrouter/openai/o1-mini-2024-09-12',\n",
       " 'openrouter/openai/o1-preview',\n",
       " 'openrouter/openai/o1-preview-2024-09-12',\n",
       " 'openrouter/openai/o3-mini',\n",
       " 'openrouter/openai/o3-mini-high',\n",
       " 'openrouter/openai/gpt-4o',\n",
       " 'openrouter/openai/gpt-4o-2024-05-13',\n",
       " 'openrouter/openai/gpt-4-vision-preview',\n",
       " 'openrouter/openai/gpt-3.5-turbo',\n",
       " 'openrouter/openai/gpt-3.5-turbo-16k',\n",
       " 'openrouter/openai/gpt-4',\n",
       " 'openrouter/anthropic/claude-instant-v1',\n",
       " 'openrouter/anthropic/claude-2',\n",
       " 'openrouter/anthropic/claude-3-opus',\n",
       " 'openrouter/google/palm-2-chat-bison',\n",
       " 'openrouter/google/palm-2-codechat-bison',\n",
       " 'openrouter/meta-llama/llama-2-13b-chat',\n",
       " 'openrouter/meta-llama/llama-2-70b-chat',\n",
       " 'openrouter/meta-llama/codellama-34b-instruct',\n",
       " 'openrouter/nousresearch/nous-hermes-llama2-13b',\n",
       " 'openrouter/mancer/weaver',\n",
       " 'openrouter/gryphe/mythomax-l2-13b',\n",
       " 'openrouter/jondurbin/airoboros-l2-70b-2.1',\n",
       " 'openrouter/undi95/remm-slerp-l2-13b',\n",
       " 'openrouter/pygmalionai/mythalion-13b',\n",
       " 'openrouter/mistralai/mistral-7b-instruct',\n",
       " 'openrouter/mistralai/mistral-7b-instruct:free',\n",
       " 'openrouter/qwen/qwen-2.5-coder-32b-instruct',\n",
       " 'gemini/gemini-2.5-pro-exp-03-25',\n",
       " 'gemini/gemini-2.5-pro',\n",
       " 'gemini/gemini-2.5-flash',\n",
       " 'gemini/gemini-2.5-flash-preview-tts',\n",
       " 'gemini/gemini-2.5-flash-preview-05-20',\n",
       " 'gemini/gemini-2.5-flash-preview-04-17',\n",
       " 'gemini/gemini-2.5-flash-lite-preview-06-17',\n",
       " 'gemini/gemini-2.0-pro-exp-02-05',\n",
       " 'gemini/gemini-2.0-flash-preview-image-generation',\n",
       " 'gemini/gemini-2.0-flash',\n",
       " 'gemini/gemini-2.0-flash-lite',\n",
       " 'gemini/gemini-2.0-flash-001',\n",
       " 'gemini/gemini-2.5-pro-preview-tts',\n",
       " 'gemini/gemini-2.5-pro-preview-06-05',\n",
       " 'gemini/gemini-2.5-pro-preview-05-06',\n",
       " 'gemini/gemini-2.5-pro-preview-03-25',\n",
       " 'gemini/gemini-2.0-flash-exp',\n",
       " 'gemini/gemini-2.0-flash-lite-preview-02-05',\n",
       " 'gemini/gemini-2.0-flash-thinking-exp',\n",
       " 'gemini/gemini-2.0-flash-thinking-exp-01-21',\n",
       " 'gemini/gemma-3-27b-it',\n",
       " 'gemini/learnlm-1.5-pro-experimental',\n",
       " 'gemini/gemini-1.5-flash-002',\n",
       " 'gemini/gemini-1.5-flash-001',\n",
       " 'gemini/gemini-1.5-flash',\n",
       " 'gemini/gemini-1.5-flash-latest',\n",
       " 'gemini/gemini-1.5-flash-8b',\n",
       " 'gemini/gemini-1.5-flash-8b-exp-0924',\n",
       " 'gemini/gemini-exp-1114',\n",
       " 'gemini/gemini-exp-1206',\n",
       " 'gemini/gemini-1.5-flash-exp-0827',\n",
       " 'gemini/gemini-1.5-flash-8b-exp-0827',\n",
       " 'gemini/gemini-pro',\n",
       " 'gemini/gemini-1.5-pro',\n",
       " 'gemini/gemini-1.5-pro-002',\n",
       " 'gemini/gemini-1.5-pro-001',\n",
       " 'gemini/gemini-1.5-pro-exp-0801',\n",
       " 'gemini/gemini-1.5-pro-exp-0827',\n",
       " 'gemini/gemini-1.5-pro-latest',\n",
       " 'gemini/gemini-pro-vision',\n",
       " 'gemini/gemini-gemma-2-27b-it',\n",
       " 'gemini/gemini-gemma-2-9b-it',\n",
       " 'perplexity/codellama-34b-instruct',\n",
       " 'perplexity/codellama-70b-instruct',\n",
       " 'perplexity/llama-3.1-70b-instruct',\n",
       " 'perplexity/llama-3.1-8b-instruct',\n",
       " 'perplexity/llama-3.1-sonar-huge-128k-online',\n",
       " 'perplexity/llama-3.1-sonar-large-128k-online',\n",
       " 'perplexity/llama-3.1-sonar-large-128k-chat',\n",
       " 'perplexity/llama-3.1-sonar-small-128k-chat',\n",
       " 'perplexity/llama-3.1-sonar-small-128k-online',\n",
       " 'perplexity/pplx-7b-chat',\n",
       " 'perplexity/pplx-70b-chat',\n",
       " 'perplexity/pplx-7b-online',\n",
       " 'perplexity/pplx-70b-online',\n",
       " 'perplexity/llama-2-70b-chat',\n",
       " 'perplexity/mistral-7b-instruct',\n",
       " 'perplexity/mixtral-8x7b-instruct',\n",
       " 'perplexity/sonar-small-chat',\n",
       " 'perplexity/sonar-small-online',\n",
       " 'perplexity/sonar-medium-chat',\n",
       " 'perplexity/sonar-medium-online',\n",
       " 'perplexity/sonar',\n",
       " 'perplexity/sonar-pro',\n",
       " 'perplexity/sonar-reasoning',\n",
       " 'perplexity/sonar-reasoning-pro',\n",
       " 'perplexity/sonar-deep-research']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litellm.get_valid_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7548809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt-4o-2024-11-20', 'openai', None, None)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litellm.get_llm_provider(\"gpt-4o-2024-11-20\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grasp-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
